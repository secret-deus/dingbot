# LLM提供商配置示例
# 复制为 .env 文件并根据实际情况配置

# ===========================================
# Ollama配置 (推荐用于本地部署)
# ===========================================

# Ollama服务地址
OLLAMA_BASE_URL=http://localhost:11434

# 使用的模型名称
# 推荐模型：
# - llama3.1:8b (通用，较好的代码生成能力)
# - llama3.1:70b (更强大，需要更多资源)
# - codellama:13b (专门用于代码生成)
# - mistral:7b (轻量级，快速响应)
# - qwen2.5:14b (中文支持较好)
OLLAMA_MODEL=llama3.1:8b

# ===========================================
# OpenAI配置 (备用选项)
# ===========================================

# OpenAI API密钥
# OPENAI_API_KEY=sk-your-openai-api-key-here

# OpenAI模型选择
# OPENAI_MODEL=gpt-3.5-turbo

# ===========================================
# Claude配置 (Anthropic)
# ===========================================

# Claude API密钥
# CLAUDE_API_KEY=your-claude-api-key-here

# Claude模型选择
# CLAUDE_MODEL=claude-3-sonnet-20240229

# ===========================================
# 其他本地LLM配置
# ===========================================

# LocalAI配置
# LOCALAI_BASE_URL=http://localhost:8080
# LOCALAI_MODEL=your-local-model

# vLLM配置
# VLLM_BASE_URL=http://localhost:8000
# VLLM_MODEL=your-vllm-model

# ===========================================
# 模型选择优先级
# ===========================================
# 系统会按以下顺序尝试：
# 1. Ollama (如果配置且可用)
# 2. OpenAI (如果有API密钥)
# 3. Claude (如果有API密钥)
# 4. 模拟模式 (总是可用的回退选项)

# ===========================================
# Ollama模型推荐和特点
# ===========================================

# 🏆 推荐组合：
# OLLAMA_MODEL=llama3.1:8b        # 平衡性能和资源消耗
# OLLAMA_MODEL=qwen2.5:14b        # 中文任务更佳
# OLLAMA_MODEL=codellama:13b       # 代码生成专用

# 💡 模型特点说明：
# 
# llama3.1:8b (8GB VRAM)
# ✅ 通用性强，代码生成能力好
# ✅ 结构化输出支持较好  
# ✅ 资源需求适中
# ❌ 中文理解相对较弱
#
# qwen2.5:14b (14GB VRAM)  
# ✅ 中文理解和生成优秀
# ✅ 代码能力也不错
# ✅ 支持更长的上下文
# ❌ 资源需求较高
#
# codellama:13b (13GB VRAM)
# ✅ 代码生成专业性最强
# ✅ 支持多种编程语言
# ✅ 代码注释和文档生成好
# ❌ 通用对话能力较弱
#
# mistral:7b (7GB VRAM)
# ✅ 推理速度快
# ✅ 资源需求低
# ✅ 多语言支持
# ❌ 复杂任务处理能力有限

# ===========================================
# 工具调用兼容性说明
# ===========================================

# ⚠️ 重要提醒：
# 
# 1. 原生工具调用支持：
#    ❌ 大部分Ollama模型不支持OpenAI风格的工具调用
#    ✅ 但支持结构化输出和JSON模式
#
# 2. 我们的解决方案：
#    🔧 通过Prompt Engineering实现"伪工具调用"
#    🔧 使用JSON格式约束输出结构  
#    🔧 多层验证确保输出质量
#
# 3. 实际效果：
#    ✅ 能够实现类似工具调用的功能
#    ✅ 生成质量往往不亚于真实工具调用
#    ✅ 完全兼容现有MCP架构

# ===========================================
# 性能优化建议
# ===========================================

# 🚀 Ollama性能优化：
# 1. 使用GPU加速：确保Ollama能访问GPU
# 2. 调整并发数：根据硬件配置适当调整
# 3. 模型预加载：启动时预热模型
# 4. 缓存策略：对常见请求进行缓存

# 💾 内存管理：
# - 8B模型：约需要8-12GB VRAM
# - 13B模型：约需要13-18GB VRAM  
# - 70B模型：约需要70-80GB VRAM (多GPU)

# ⚡ 响应速度：
# - 本地模型：2-10秒 (取决于硬件)
# - OpenAI API：1-3秒 (取决于网络)
# - 模拟模式：< 1秒 (即时响应) 