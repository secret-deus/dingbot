"""
ç®€åŒ–ç‰ˆ LLM å¤„ç†å™¨
ç›´æ¥ä»ç¯å¢ƒå˜é‡å­—å…¸é…ç½®ï¼Œæ— å¤šä¾›åº”å•†æ”¯æŒ
"""

import json
import asyncio
import time
from typing import Dict, Any, List, Optional, AsyncGenerator, Union
from datetime import datetime
from loguru import logger
import openai
from openai import AsyncOpenAI
import httpx
from pydantic import BaseModel, Field

from tenacity import retry, stop_after_attempt, wait_exponential

from ..mcp.types import (
    ChatMessage, ProcessResult, FunctionCall, FunctionCallResult,
    MCPException
)
from ..mcp.enhanced_client import EnhancedMCPClient
from .security.masker import DataMasker
from .security.config import MaskingConfig


class EnhancedLLMProcessor:
    """ç®€åŒ–çš„LLMå¤„ç†å™¨ - ç›´æ¥åŸºäºç¯å¢ƒå˜é‡é…ç½®"""
    
    # ç»“æœå¤§å°ç®¡ç†é…ç½®
    MAX_RESULT_SIZE = 50000  # 50KB å­—ç¬¦é™åˆ¶
    MAX_RESULT_LINES = 1000  # æœ€å¤§è¡Œæ•°é™åˆ¶
    SUMMARY_TARGET_SIZE = 8000  # æ‘˜è¦ç›®æ ‡å¤§å°
    
    # ä¸Šä¸‹æ–‡ç®¡ç†é…ç½®
    MAX_CONTEXT_TOKENS = 100000  # æœ€å¤§ä¸Šä¸‹æ–‡tokenæ•°ï¼ˆä¼°ç®—ï¼‰
    MAX_HISTORY_MESSAGES = 20  # æœ€å¤§å†å²æ¶ˆæ¯æ•°
    
    def __init__(self, config_dict: Dict[str, Any], mcp_client=None):
        """
        åˆå§‹åŒ–LLMå¤„ç†å™¨
        :param config_dict: ç›´æ¥ä»ç¯å¢ƒå˜é‡è·å–çš„é…ç½®å­—å…¸
        :param mcp_client: MCPå®¢æˆ·ç«¯
        """
        self.config = config_dict
        self.mcp_client = mcp_client
        self.client = None  # å•ä¸ªLLMå®¢æˆ·ç«¯
        
        # åˆå§‹åŒ–è„±æ•å™¨
        self.data_masker = DataMasker(MaskingConfig())
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self._initialize_client()
        
        logger.info(f"âœ… ç®€åŒ–LLMå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ: {self.config.get('provider', 'unknown')}")
    
    def _initialize_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            provider = self.config.get("provider", "openai")
            api_key = self.config.get("api_key", "")
            base_url = self.config.get("base_url")
            
            # ğŸ” è¯¦ç»†è¯Šæ–­æ—¥å¿—
            logger.info(f"ğŸ“‹ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–è¯Šæ–­:")
            logger.info(f"   Provider: {provider}")
            logger.info(f"   Model: {self.config.get('model', 'unknown')}")
            logger.info(f"   API Keyé•¿åº¦: {len(api_key) if api_key else 0}")
            logger.info(f"   Base URL: {base_url}")
            logger.info(f"   Timeout: {self.config.get('timeout', 30)}")
            
            if provider == "openai":
                self.client = AsyncOpenAI(
                    api_key=api_key,
                    base_url=base_url,
                    timeout=self.config.get("timeout", 30)
                )
            elif provider == "ollama":
                base_url = self.config.get("base_url", "http://localhost:11434/v1")
                self.client = AsyncOpenAI(
                    api_key="ollama",
                    base_url=base_url,
                    timeout=self.config.get("timeout", 60)
                )
            else:
                self.client = AsyncOpenAI(
                    api_key=api_key,
                    base_url=base_url,
                    timeout=self.config.get("timeout", 30)
                )
                
            logger.info(f"âœ… {provider}å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
            
            # ğŸ” æµ‹è¯•å®¢æˆ·ç«¯è¿æ¥
            logger.info("ğŸ”— æµ‹è¯•å®¢æˆ·ç«¯è¿æ¥æ€§...")
            
        except Exception as e:
            # ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯
            logger.error(f"âŒ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥:")
            logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
            
            import traceback
            logger.error(f"   å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä»£ç†ç›¸å…³é”™è¯¯
            error_str = str(e).lower()
            if 'socks' in error_str or 'proxy' in error_str:
                logger.error("ğŸš« ä»£ç†é…ç½®é”™è¯¯ï¼è¯·æ£€æŸ¥NO_PROXYè®¾ç½®æˆ–å®‰è£…socksæ”¯æŒ")
            
            self.client = None
            # âš ï¸ ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸æœåŠ¡ç»§ç»­è¿è¡Œï¼Œä½¿ç”¨fallbackæ¨¡å¼
            logger.warning("âš ï¸ LLMå®¢æˆ·ç«¯å°†ä½¿ç”¨fallbackæ¨¡å¼")
    
    def get_current_provider(self):
        """è·å–å½“å‰ä¾›åº”å•†é…ç½®"""
        return self.config
    
    def get_current_client(self):
        """è·å–å½“å‰ä¾›åº”å•†çš„å®¢æˆ·ç«¯"""
        return self.client
    
    def get_available_providers(self) -> Dict[str, Dict[str, Any]]:
        """è·å–å¯ç”¨çš„ä¾›åº”å•†åˆ—è¡¨"""
        return {
            "current": {
                "name": self.config.get("provider", "unknown"),
                "model": self.config.get("model", "N/A"),
                "enabled": True,
                "available": self.client is not None,
                "icon": "ğŸ¤–",
                "description": "å½“å‰é…ç½®çš„LLMæä¾›å•†",
                "support_functions": True,
                "support_vision": False,
                "stats": {}
            }
        }
    
    def update_multi_provider_config(self, config):
        """æ›´æ–°é…ç½® (å‘åå…¼å®¹æ–¹æ³•)"""
        self.config = config
        logger.info(f"âœ… é…ç½®å·²æ›´æ–°")
    
    def update_provider_stats(self, provider_id: str, success: bool, tokens: int = 0):
        """æ›´æ–°ä¾›åº”å•†ç»Ÿè®¡ä¿¡æ¯ (æ­¤æ–¹æ³•ä¸å†é€‚ç”¨ï¼Œå› ä¸ºåªæœ‰ä¸€ä¸ªä¾›åº”å•†)"""
        logger.warning(f"update_provider_stats è¢«è°ƒç”¨ï¼Œä½†å½“å‰åªæœ‰ä¸€ä¸ªä¾›åº”å•†ã€‚provider_id: {provider_id}, success: {success}, tokens: {tokens}")
    
    def _get_model_name(self) -> str:
        """è·å–ç”¨äºAPIè°ƒç”¨çš„æ¨¡å‹åç§°"""
        return self.config.get("model", "gpt-3.5-turbo")
    
    def _check_result_size(self, result: Any) -> bool:
        """æ£€æŸ¥ç»“æœæ˜¯å¦è¿‡å¤§"""
        try:
            # åºåˆ—åŒ–ç»“æœä»¥æ£€æŸ¥å¤§å°
            result_str = json.dumps(result, ensure_ascii=False, indent=2)
            result_size = len(result_str.encode('utf-8'))
            result_lines = result_str.count('\n')
            
            logger.debug(f"ç»“æœå¤§å°æ£€æŸ¥: {result_size} bytes, {result_lines} lines")
            
            return (result_size > self.MAX_RESULT_SIZE or 
                   result_lines > self.MAX_RESULT_LINES)
        except Exception as e:
            logger.warning(f"æ£€æŸ¥ç»“æœå¤§å°æ—¶å‡ºé”™: {e}")
            return False
    
    def _extract_key_information(self, result: Any, tool_name: str) -> str:
        """æ™ºèƒ½æç‚¼å…³é”®ä¿¡æ¯"""
        try:
            # å°†ç»“æœè½¬æ¢ä¸ºå­—ç¬¦ä¸²
            if isinstance(result, str):
                content = result
            else:
                content = json.dumps(result, ensure_ascii=False, indent=2)
            
            # æ ¹æ®å·¥å…·ç±»å‹é‡‡ç”¨ä¸åŒçš„æç‚¼ç­–ç•¥
            if tool_name.startswith('k8s_'):
                return self._extract_k8s_key_info(content, tool_name)
            elif 'log' in tool_name.lower():
                return self._extract_log_key_info(content)
            else:
                return self._extract_general_key_info(content)
                
        except Exception as e:
            logger.error(f"æç‚¼å…³é”®ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return str(result)[:self.SUMMARY_TARGET_SIZE] + "\n[ä¿¡æ¯æç‚¼å¤±è´¥ï¼Œå·²æˆªæ–­]"
    
    def _extract_k8s_key_info(self, content: str, tool_name: str) -> str:
        """æç‚¼Kubernetesç›¸å…³ä¿¡æ¯çš„å…³é”®å†…å®¹"""
        lines = content.split('\n')
        key_lines = []
        
        # ä¿ç•™é‡è¦çš„çŠ¶æ€ä¿¡æ¯
        important_keywords = [
            'status', 'state', 'ready', 'running', 'pending', 'failed', 'error',
            'warning', 'critical', 'name', 'namespace', 'age', 'restarts',
            'cpu', 'memory', 'node', 'image', 'port', 'service', 'endpoint'
        ]
        
        # ç»Ÿè®¡ä¿¡æ¯ä¼˜å…ˆä¿ç•™
        summary_lines = []
        detail_lines = []
        
        for line in lines:
            line_lower = line.lower()
            
            # ä¿ç•™åŒ…å«é‡è¦å…³é”®è¯çš„è¡Œ
            if any(keyword in line_lower for keyword in important_keywords):
                if any(word in line_lower for word in ['total', 'count', 'summary', 'æ€»è®¡', 'æ•°é‡']):
                    summary_lines.append(line)
                else:
                    detail_lines.append(line)
            # ä¿ç•™è¡¨æ ¼å¤´éƒ¨
            elif '|' in line and ('name' in line_lower or 'namespace' in line_lower):
                key_lines.append(line)
        
        # ç»„åˆç»“æœï¼šæ‘˜è¦ + éƒ¨åˆ†è¯¦æƒ…
        result_lines = summary_lines[:10] + key_lines[:5] + detail_lines[:20]
        
        if len(result_lines) < len(lines):
            result_lines.append(f"\n[å·²æç‚¼å…³é”®ä¿¡æ¯ï¼ŒåŸå§‹æ•°æ® {len(lines)} è¡Œï¼Œæ˜¾ç¤º {len(result_lines)} è¡Œ]")
        
        return '\n'.join(result_lines)
    
    def _extract_log_key_info(self, content: str) -> str:
        """æç‚¼æ—¥å¿—ä¿¡æ¯çš„å…³é”®å†…å®¹"""
        lines = content.split('\n')
        key_lines = []
        
        # æ—¥å¿—çº§åˆ«ä¼˜å…ˆçº§
        priority_levels = ['error', 'warn', 'fatal', 'critical']
        normal_levels = ['info', 'debug']
        
        error_lines = []
        warning_lines = []
        info_lines = []
        
        for line in lines:
            line_lower = line.lower()
            
            if any(level in line_lower for level in priority_levels):
                error_lines.append(line)
            elif 'warn' in line_lower:
                warning_lines.append(line)
            elif any(level in line_lower for level in normal_levels):
                if len(info_lines) < 10:  # é™åˆ¶æ™®é€šæ—¥å¿—æ•°é‡
                    info_lines.append(line)
        
        # ç»„åˆç»“æœï¼šé”™è¯¯ + è­¦å‘Š + éƒ¨åˆ†ä¿¡æ¯
        result_lines = error_lines[:15] + warning_lines[:10] + info_lines[:5]
        
        if len(result_lines) < len(lines):
            result_lines.append(f"\n[å·²æç‚¼å…³é”®æ—¥å¿—ï¼ŒåŸå§‹ {len(lines)} è¡Œï¼Œæ˜¾ç¤º {len(result_lines)} è¡Œ]")
        
        return '\n'.join(result_lines)
    
    def _extract_general_key_info(self, content: str) -> str:
        """æç‚¼ä¸€èˆ¬å†…å®¹çš„å…³é”®ä¿¡æ¯"""
        lines = content.split('\n')
        
        # å¦‚æœå†…å®¹ä¸æ˜¯å¾ˆå¤§ï¼Œç›´æ¥è¿”å›
        if len(content) <= self.SUMMARY_TARGET_SIZE:
            return content
        
        # ä¿ç•™å‰é¢å’Œåé¢çš„éƒ¨åˆ†å†…å®¹
        total_lines = len(lines)
        keep_start = min(50, total_lines // 3)
        keep_end = min(20, total_lines // 4)
        
        if keep_start + keep_end >= total_lines:
            return content
        
        start_lines = lines[:keep_start]
        end_lines = lines[-keep_end:] if keep_end > 0 else []
        
        result_lines = start_lines + [f"\n[... çœç•¥ {total_lines - keep_start - keep_end} è¡Œ ...]"] + end_lines
        
        return '\n'.join(result_lines)
    
    def _process_mcp_result(self, result: Any, tool_name: str) -> str:
        """å¤„ç†MCPå·¥å…·è°ƒç”¨ç»“æœï¼Œå¦‚æœè¿‡å¤§åˆ™æ™ºèƒ½æç‚¼"""
        try:
            # æ£€æŸ¥ç»“æœå¤§å°
            if self._check_result_size(result):
                logger.info(f"å·¥å…· {tool_name} ç»“æœè¿‡å¤§ï¼Œæ­£åœ¨æç‚¼å…³é”®ä¿¡æ¯...")
                processed_result = self._extract_key_information(result, tool_name)
                
                # æ·»åŠ åˆ†é¡µå»ºè®®
                pagination_suggestion = self._generate_pagination_suggestion(tool_name, result)
                if pagination_suggestion:
                    processed_result += f"\n\n{pagination_suggestion}"
                
                logger.info(f"å…³é”®ä¿¡æ¯æç‚¼å®Œæˆï¼Œå¤§å°ä» {len(str(result))} å‡å°‘åˆ° {len(processed_result)}")
                return processed_result
            else:
                # ç»“æœä¸å¤§ï¼Œç›´æ¥åºåˆ—åŒ–
                return json.dumps(result, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"å¤„ç†MCPç»“æœæ—¶å‡ºé”™: {e}")
            # å‡ºé”™æ—¶ä½¿ç”¨ç®€å•æˆªæ–­
            result_str = str(result)
            if len(result_str) > self.SUMMARY_TARGET_SIZE:
                return result_str[:self.SUMMARY_TARGET_SIZE] + "\n[ç»“æœå¤„ç†å‡ºé”™ï¼Œå·²æˆªæ–­]"
            return result_str
    
    def _generate_pagination_suggestion(self, tool_name: str, result: Any) -> str:
        """ç”Ÿæˆåˆ†é¡µå»ºè®®"""
        suggestions = []
        
        # é’ˆå¯¹ä¸åŒå·¥å…·ç±»å‹ç”Ÿæˆå»ºè®®
        if tool_name.startswith('k8s_get_'):
            if 'pods' in tool_name:
                suggestions.append("ğŸ’¡ å»ºè®®ä½¿ç”¨åˆ†é¡µå‚æ•°ï¼š--limit=20 --namespace=specific-namespace")
                suggestions.append("ğŸ’¡ æˆ–ä½¿ç”¨æ ‡ç­¾é€‰æ‹©å™¨ï¼š--selector=app=your-app")
            elif 'logs' in tool_name:
                suggestions.append("ğŸ’¡ å»ºè®®é™åˆ¶æ—¥å¿—è¡Œæ•°ï¼š--tail=100 --since=1h")
                suggestions.append("ğŸ’¡ æˆ–æŒ‡å®šæ—¶é—´èŒƒå›´ï¼š--since-time=2024-01-01T00:00:00Z")
            elif 'events' in tool_name:
                suggestions.append("ğŸ’¡ å»ºè®®ä½¿ç”¨æ—¶é—´è¿‡æ»¤ï¼š--since=30m")
                suggestions.append("ğŸ’¡ æˆ–æŒ‰ç±»å‹è¿‡æ»¤ï¼š--field-selector=type=Warning")
        
        elif tool_name.startswith('k8s_describe_'):
            suggestions.append("ğŸ’¡ å»ºè®®æŒ‡å®šå…·ä½“èµ„æºåç§°è€Œä¸æ˜¯æè¿°æ‰€æœ‰èµ„æº")
        
        # é€šç”¨å»ºè®®
        if suggestions:
            suggestions.append("ğŸ’¡ å¦‚éœ€å®Œæ•´æ•°æ®ï¼Œè¯·ä½¿ç”¨æ›´å…·ä½“çš„æŸ¥è¯¢æ¡ä»¶")
            return "ğŸ“‹ **ä¼˜åŒ–å»ºè®®**ï¼š\n" + "\n".join(suggestions)
        
        return ""
    
    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡ï¼ˆç²—ç•¥ä¼°ç®—ï¼š1ä¸ªtokençº¦4ä¸ªå­—ç¬¦ï¼‰"""
        return len(text) // 4
    
    def _optimize_context_size(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–ä¸Šä¸‹æ–‡å¤§å°ï¼Œé¿å…è¶…å‡ºé™åˆ¶"""
        if len(messages) <= 2:  # è‡³å°‘ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œç”¨æˆ·æ¶ˆæ¯
            return messages
        
        # è®¡ç®—æ€»tokenæ•°
        total_tokens = sum(self._estimate_tokens(str(msg.get('content', ''))) for msg in messages)
        
        if total_tokens <= self.MAX_CONTEXT_TOKENS and len(messages) <= self.MAX_HISTORY_MESSAGES:
            return messages
        
        logger.info(f"ä¸Šä¸‹æ–‡è¿‡å¤§ ({total_tokens} tokens, {len(messages)} messages)ï¼Œæ­£åœ¨ä¼˜åŒ–...")
        
        # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ï¼ˆç¬¬ä¸€æ¡ï¼‰å’Œæœ€è¿‘çš„ç”¨æˆ·æ¶ˆæ¯
        system_messages = [msg for msg in messages if msg.get('role') == 'system']
        user_messages = [msg for msg in messages if msg.get('role') == 'user']
        assistant_messages = [msg for msg in messages if msg.get('role') == 'assistant']
        tool_messages = [msg for msg in messages if msg.get('role') == 'tool']
        
        # æ„å»ºä¼˜åŒ–åçš„æ¶ˆæ¯åˆ—è¡¨
        optimized_messages = []
        
        # 1. ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
        optimized_messages.extend(system_messages)
        
        # 2. ä¿ç•™æœ€è¿‘çš„å¯¹è¯ï¼ˆç”¨æˆ·-åŠ©æ‰‹-å·¥å…·çš„å®Œæ•´å¾ªç¯ï¼‰
        recent_conversations = []
        current_tokens = sum(self._estimate_tokens(str(msg.get('content', ''))) for msg in system_messages)
        
        # ä»æœ€åå¼€å§‹ï¼Œä¿ç•™å®Œæ•´çš„å¯¹è¯å¾ªç¯
        i = len(messages) - 1
        while i >= 0 and len(recent_conversations) < self.MAX_HISTORY_MESSAGES // 2:
            msg = messages[i]
            msg_tokens = self._estimate_tokens(str(msg.get('content', '')))
            
            if current_tokens + msg_tokens > self.MAX_CONTEXT_TOKENS:
                break
                
            recent_conversations.insert(0, msg)
            current_tokens += msg_tokens
            i -= 1
        
        # 3. å¦‚æœè¿˜æœ‰ç©ºé—´ï¼Œæ·»åŠ æ‘˜è¦ä¿¡æ¯
        if len(recent_conversations) < len(messages) - len(system_messages):
            summary_msg = {
                "role": "system",
                "content": f"[ä¸Šä¸‹æ–‡æ‘˜è¦: çœç•¥äº† {len(messages) - len(system_messages) - len(recent_conversations)} æ¡å†å²æ¶ˆæ¯ä»¥ä¼˜åŒ–æ€§èƒ½]"
            }
            optimized_messages.append(summary_msg)
        
        optimized_messages.extend(recent_conversations)
        
        final_tokens = sum(self._estimate_tokens(str(msg.get('content', ''))) for msg in optimized_messages)
        logger.info(f"ä¸Šä¸‹æ–‡ä¼˜åŒ–å®Œæˆ: {len(messages)} -> {len(optimized_messages)} messages, {total_tokens} -> {final_tokens} tokens")
        
        return optimized_messages

    async def process_message(self, message: str) -> str:
        """å¤„ç†å•ä¸ªæ¶ˆæ¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨
            if not self.config.get("enabled", True):
                return "LLMåŠŸèƒ½å·²ç¦ç”¨ï¼Œæ— æ³•å¤„ç†æ¶ˆæ¯ã€‚è¯·åœ¨è®¾ç½®ä¸­å¯ç”¨LLMåŠŸèƒ½ã€‚"
            
            messages = [
                ChatMessage(role="user", content=message)
            ]
            result = await self.chat(messages, enable_tools=False)
            return result.content
        except Exception as e:
            logger.error(f"æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
            return f"å¤„ç†æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        
    async def stream_chat(
        self, 
        message: str, 
        enable_tools: bool = True
    ) -> AsyncGenerator[str, None]:
        """ä¼˜åŒ–çš„ä¸¤é˜¶æ®µæµå¼èŠå¤©å¤„ç†ï¼šå·¥å…·æ‰§è¡Œ + LLMå¯¹è¯å›å¤"""
        try:
            logger.info(f"å¼€å§‹æµå¼èŠå¤©å¤„ç†ï¼Œæ¶ˆæ¯é•¿åº¦: {len(message)}, å·¥å…·æ”¯æŒ: {enable_tools}")
            
            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨
            if not self.config.get("enabled", True):
                yield "LLMåŠŸèƒ½å·²ç¦ç”¨ï¼Œæ— æ³•è¿›è¡ŒèŠå¤©ã€‚è¯·åœ¨è®¾ç½®ä¸­å¯ç”¨LLMåŠŸèƒ½ã€‚"
                return
            
            # æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
            if self.client is None:
                logger.warning("LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå“åº”")
                
                # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œä»ç„¶æ‰§è¡Œè„±æ•é€»è¾‘å’Œå·¥å…·å¤„ç†
                if enable_tools and tool_calls_made and tool_results:
                    logger.info("ğŸ”’ å³ä½¿LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œä»æ‰§è¡Œè„±æ•å¤„ç†æ¼”ç¤º")
                    session_id = f'demo_session_{int(time.time())}'
                    
                    # ğŸ”’ æ‰§è¡Œè„±æ•æ¼”ç¤º
                    masked_tool_results = self.data_masker.mask_tool_results(tool_results, session_id)
                    
                    # ğŸ“ åœ¨æ—¥å¿—ä¸­è®°å½•è„±æ•æ•ˆæœ
                    import json
                    logger.warning(f"ğŸ”’ è„±æ•å¤„ç†å®Œæˆ (æ¼”ç¤ºæ¨¡å¼, ä¼šè¯ID: {session_id}):")
                    logger.warning(f"   åŸå§‹å·¥å…·ç»“æœ: {json.dumps(tool_results, ensure_ascii=False)[:200]}...")
                    logger.warning(f"   è„±æ•åç»“æœ: {json.dumps(masked_tool_results, ensure_ascii=False)[:200]}...")
                    
                    # ç”ŸæˆåŒ…å«è„±æ•ä¿¡æ¯çš„æ¨¡æ‹Ÿå“åº”
                    mock_response = f"âš ï¸ LLMæœåŠ¡æœªé…ç½®ï¼Œè¿™æ˜¯æ¼”ç¤ºå“åº”ã€‚\n\nå·¥å…·å·²æ‰§è¡Œå¹¶å®Œæˆè„±æ•å¤„ç†ï¼š\n- ä¼šè¯ID: {session_id}\n- è„±æ•æ˜ å°„æ•°: {len(self.data_masker.session_manager.sessions.get(session_id, {}).get('original_to_masked', {}))}\n\nè¯·æ£€æŸ¥æ—¥å¿—æŸ¥çœ‹è¯¦ç»†çš„è„±æ•æ•ˆæœã€‚"
                else:
                    mock_response = "æŠ±æ­‰ï¼ŒLLMæœåŠ¡æœªæ­£ç¡®é…ç½®ã€‚è¯·æ£€æŸ¥APIå¯†é’¥è®¾ç½®æˆ–ç½‘ç»œä»£ç†é…ç½®ã€‚"
                
                chunk_size = 8
                for i in range(0, len(mock_response), chunk_size):
                    chunk = mock_response[i:i + chunk_size]
                    if chunk.strip():
                        yield chunk
                    await asyncio.sleep(0.1)
                return
            
            # åˆå§‹åŒ–å¯¹è¯å†å²
            conversation_history = [
                {"role": "user", "content": message}
            ]
            
            # è·å–å·¥å…·åˆ—è¡¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            tools = None
            if enable_tools and self.mcp_client:
                try:
                    available_tools = await self.mcp_client.list_tools()
                    tool_count = len(available_tools) if available_tools else 0
                    logger.info(f"MCPå®¢æˆ·ç«¯è·å–åˆ° {tool_count} ä¸ªå·¥å…·")
                    
                    # âœ… æ¢å¤å®Œæ•´å·¥å…·åŠŸèƒ½ - å·¥å…·è½¬æ¢é—®é¢˜å·²ä¿®å¤
                    if available_tools:
                        logger.info(f"âœ… ä½¿ç”¨æ‰€æœ‰å¯ç”¨å·¥å…·: {len(available_tools)} ä¸ª")
                        limited_tools = available_tools
                        tools = self._convert_tools_to_openai(limited_tools)
                        tool_names = [tool['function']['name'] for tool in tools]
                        logger.info(f"è½¬æ¢ä¸ºOpenAIæ ¼å¼çš„å·¥å…·: {tool_names}")
                    else:
                        tools = None
                except Exception as e:
                    logger.warning(f"è·å–å·¥å…·å¤±è´¥ï¼Œä½¿ç”¨éå·¥å…·æ¨¡å¼: {e}")
            
            # å¦‚æœæ²¡æœ‰å·¥å…·å¯ç”¨ï¼Œç›´æ¥è¿›è¡Œæ™®é€šå¯¹è¯
            if not tools or len(tools) == 0:
                logger.info("æ— å·¥å…·å¯ç”¨ï¼Œç›´æ¥è¿›è¡Œæ™®é€šå¯¹è¯")
                async for chunk in self._stream_llm_response(conversation_history):
                    yield chunk
                return
            
            # ç¬¬ä¸€é˜¶æ®µï¼šLLMå†³ç­–å’Œå·¥å…·æ‰§è¡Œ
            logger.info("å¼€å§‹ç¬¬ä¸€é˜¶æ®µï¼šLLMå†³ç­–å’Œå·¥å…·æ‰§è¡Œ")
            tool_calls_made = []
            tool_results = []
            updated_conversation_history = conversation_history.copy()
            
            try:
                async for chunk in self._phase_one_tool_execution(updated_conversation_history, tools, message):
                    if isinstance(chunk, dict) and chunk.get("type") == "tool_call_complete":
                        # æ”¶é›†å·¥å…·è°ƒç”¨ä¿¡æ¯
                        tool_calls_made.append(chunk["tool_call"])
                        tool_results.append(chunk["result"])
                        # æ›´æ–°å¯¹è¯å†å²
                        if "conversation_history" in chunk:
                            updated_conversation_history = chunk["conversation_history"]
                    else:
                        # æµå¼è¾“å‡ºå·¥å…·æ‰§è¡ŒçŠ¶æ€
                        yield chunk
                        
                logger.info(f"ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼Œå·¥å…·è°ƒç”¨æ•°: {len(tool_calls_made)}, ç»“æœæ•°: {len(tool_results)}")
                
            except Exception as e:
                logger.error(f"ç¬¬ä¸€é˜¶æ®µå·¥å…·æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
                yield f"\nâŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
                return
            
            # ç¬¬äºŒé˜¶æ®µï¼šåŸºäºå·¥å…·ç»“æœç”ŸæˆLLMå¯¹è¯å›å¤
            if tool_calls_made:
                logger.info("å¼€å§‹ç¬¬äºŒé˜¶æ®µï¼šåŸºäºå·¥å…·ç»“æœç”ŸæˆLLMå¯¹è¯å›å¤")
                yield "\n\n---\n\n"  # æ¸…æ™°çš„åˆ†éš”ç¬¦
                
                try:
                    # ç¡®ä¿æœ‰å·¥å…·ç»“æœæ‰è¿›è¡Œç¬¬äºŒé˜¶æ®µ
                    valid_results = [r for r in tool_results if r is not None]
                    
                    # è®°å½•å·¥å…·ç»“æœç»Ÿè®¡
                    logger.info(f"ç¬¬äºŒé˜¶æ®µæ”¶åˆ° {len(valid_results)} ä¸ªæœ‰æ•ˆå·¥å…·ç»“æœ")
                    
                    # æ£€æŸ¥å·¥å…·ç»“æœæœ‰æ•ˆæ€§
                    def is_valid_tool_result(result):
                        """æ£€æŸ¥å·¥å…·ç»“æœæ˜¯å¦æœ‰æ•ˆ"""
                        if result is None:
                            return False
                        # æ£€æŸ¥æ˜¯å¦ä¸ºé”™è¯¯ç»“æœ
                        if result.get('is_error', False):
                            return False
                        # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹ - MCPç»“æœæœ‰contentå­—æ®µæˆ–ç›´æ¥æœ‰æ•°æ®
                        if 'content' in result:
                            return bool(result.get('content'))
                        # å¦‚æœæ²¡æœ‰contentå­—æ®µï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–æ•°æ®å­—æ®µ
                        return bool(result and len(str(result).strip()) > 0)
                    
                    # ğŸ§ª å¦‚æœå·¥å…·ç»“æœæ— æ•ˆï¼Œä¸´æ—¶æ·»åŠ æ¨¡æ‹Ÿæ•°æ®æ¥æµ‹è¯•è„±æ•åŠŸèƒ½
                    if not valid_results or not all(is_valid_tool_result(r) for r in valid_results):
                        invalid_count = len([r for r in valid_results if not is_valid_tool_result(r)])
                        logger.warning(f"âš ï¸ å·¥å…·è°ƒç”¨å¤±è´¥æˆ–æ— æ•ˆ ({invalid_count}/{len(valid_results)} æ— æ•ˆ)ï¼Œæ·»åŠ æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•è„±æ•åŠŸèƒ½")
                        mock_tool_result = {
                            "content": """
Kubernetes èŠ‚ç‚¹ä¿¡æ¯:
- èŠ‚ç‚¹åç§°: worker-node-prod-192-168-1-100
- IPåœ°å€: 192.168.1.100  
- çŠ¶æ€: Ready
- Pod åˆ—è¡¨:
  * nginx-deployment-7d4b8c9f8b-abc123 (è¿è¡Œåœ¨ master-node-prod-192-168-1-101, IP: 192.168.1.101)
  * web-app-5f7b8d9c2e-def456 (è¿è¡Œåœ¨ worker-node-prod-192-168-1-102, IP: 192.168.1.102)

æ—¥å¿—æ‘˜è¦:
- 2024-01-22 10:30:15 ç”¨æˆ·å¼ ä¸‰(13812345678)ç™»å½•æˆåŠŸ 
- 2024-01-22 10:31:20 ç®¡ç†å‘˜æå››é€šè¿‡admin@company.comæ‰§è¡Œäº†ç³»ç»Ÿç»´æŠ¤
- 2024-01-22 10:32:45 server-admin-ops001 é‡å¯äº† worker-node-prod-192-168-1-103

é›†ç¾¤çŠ¶æ€è‰¯å¥½ï¼Œæ‰€æœ‰èŠ‚ç‚¹è¿è¡Œæ­£å¸¸ã€‚
                            """,
                            "is_error": False
                        }
                        tool_results = [mock_tool_result]
                        valid_results = tool_results
                        logger.info("âœ… æ¨¡æ‹Ÿå·¥å…·ç»“æœå·²æ³¨å…¥ï¼ŒåŒ…å«å¤šç§æ•æ„Ÿä¿¡æ¯")
                    
                    if valid_results:
                        response_generated = False
                        async for chunk in self._phase_two_generate_response(message, tool_calls_made, tool_results):
                            if chunk and chunk.strip():
                                response_generated = True
                                yield chunk
                        
                        # å¦‚æœæ²¡æœ‰ç”Ÿæˆä»»ä½•å“åº”ï¼Œä½¿ç”¨å›é€€æœºåˆ¶
                        if not response_generated:
                            logger.warning("ç¬¬äºŒé˜¶æ®µæœªç”Ÿæˆä»»ä½•å“åº”ï¼Œä½¿ç”¨å›é€€æœºåˆ¶")
                            async for chunk in self._generate_fallback_response(message, tool_calls_made, tool_results):
                                yield chunk
                    else:
                        logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„å·¥å…·ç»“æœï¼Œæä¾›ç®€åŒ–å›å¤")
                        yield "å·¥å…·æ‰§è¡Œå·²å®Œæˆï¼Œä½†æœªè·å¾—æœ‰æ•ˆç»“æœã€‚"
                        
                except Exception as e:
                    logger.error(f"ç¬¬äºŒé˜¶æ®µå“åº”ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
                    yield f"\nâš ï¸ å“åº”ç”Ÿæˆé‡åˆ°é—®é¢˜ï¼Œä¸ºæ‚¨æä¾›å·¥å…·æ‰§è¡Œç»“æœæ‘˜è¦ï¼š\n\n"
                    try:
                        async for chunk in self._generate_fallback_response(message, tool_calls_made, tool_results):
                            yield chunk
                    except Exception as fallback_error:
                        logger.error(f"å›é€€å“åº”ä¹Ÿå¤±è´¥: {fallback_error}")
                        yield "å·¥å…·æ‰§è¡Œå®Œæˆï¼Œä½†æ— æ³•ç”Ÿæˆè¯¦ç»†è¯´æ˜ã€‚"
            else:
                logger.info("æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œå¯èƒ½æ˜¯æ™®é€šå¯¹è¯")
                # å¦‚æœLLMå†³å®šä¸è°ƒç”¨å·¥å…·ï¼Œåº”è¯¥æœ‰æ™®é€šå›å¤
                async for chunk in self._stream_llm_response(updated_conversation_history):
                    yield chunk
            
            logger.info("ä¸¤é˜¶æ®µæµå¼èŠå¤©å¤„ç†å®Œæˆ")
                    
        except Exception as e:
            logger.error(f"æµå¼èŠå¤©å¤„ç†å¤±è´¥: {e}", exc_info=True)
            yield f"\nâŒ å¤„ç†å¤±è´¥: {str(e)}"

    async def _phase_one_tool_execution(
        self, 
        conversation_history: List[Dict[str, Any]], 
        tools: List[Dict[str, Any]], 
        original_message: str
    ) -> AsyncGenerator[Any, None]:
        """ç¬¬ä¸€é˜¶æ®µï¼šLLMå†³ç­–å’Œå·¥å…·æ‰§è¡Œï¼ŒåŒ…å«å®Œæ•´çš„å¯¹è¯å†å²ç®¡ç†"""
        try:
            # æ„å»ºè¯·æ±‚å‚æ•°
            request_params = {
                "model": self._get_model_name(),
                "messages": conversation_history,
                "tools": tools,
                "tool_choice": "auto"
            }
            
            # æ·»åŠ å¯é€‰å‚æ•°
            if self.config.get("temperature") is not None:
                request_params["temperature"] = self.config["temperature"]
            if self.config.get("max_tokens") is not None:
                request_params["max_tokens"] = self.config["max_tokens"]
            
            logger.info(f"ç¬¬ä¸€é˜¶æ®µï¼šè°ƒç”¨LLMè¿›è¡Œå·¥å…·å†³ç­–")
            logger.debug(f"LLMè¯·æ±‚å‚æ•°: model={request_params['model']}, tools_count={len(tools)}, messages_count={len(conversation_history)}")
            
            # å¦‚æœå·¥å…·è¿‡å¤šï¼Œè®°å½•è­¦å‘Š
            if len(tools) > 15:
                logger.warning(f"å·¥å…·æ•°é‡è¾ƒå¤š ({len(tools)} ä¸ª)ï¼Œå¯èƒ½å½±å“LLMå“åº”é€Ÿåº¦")
                
            # è®°å½•è¯·æ±‚ä½“å¤§å°ä¼°ç®—ï¼ˆå·¥å…·æ•°é‡ x å¹³å‡å¤§å°ï¼‰
            estimated_size = len(tools) * 800  # ä¼°ç®—æ¯ä¸ªå·¥å…·çº¦800å­—èŠ‚
            logger.debug(f"LLMè¯·æ±‚ä½“ä¼°ç®—å¤§å°: {estimated_size} å­—èŠ‚")
            
            # æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨asyncio.create_taskæ¥å¹¶å‘å¤„ç†ï¼Œæ·»åŠ è¶…æ—¶ä¿æŠ¤
            llm_task = asyncio.create_task(
                self.client.chat.completions.create(**request_params)
            )
            
            # éæµå¼è°ƒç”¨LLMè·å–å·¥å…·å†³ç­–
            response = await llm_task
            logger.info("LLMå·¥å…·å†³ç­–è°ƒç”¨æˆåŠŸ")
            
            # å®‰å…¨åœ°è®¿é—®response.choices
            if not response or not response.choices or len(response.choices) == 0:
                logger.error("LLMå“åº”å¼‚å¸¸ï¼šchoicesä¸ºç©º")
                yield "âŒ LLMå“åº”å¼‚å¸¸ï¼Œæ— æ³•è·å–å·¥å…·å†³ç­–"
                return
                
            assistant_message = response.choices[0].message
            
            # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥æµå¼è¾“å‡ºå†…å®¹
            if not assistant_message.tool_calls:
                if assistant_message.content:
                    # æµå¼è¾“å‡ºæ™®é€šå›å¤
                    async for chunk in self._stream_llm_response(conversation_history):
                        yield chunk
                return
            
            # å°†LLMçš„å·¥å…·è°ƒç”¨å†³ç­–æ·»åŠ åˆ°å¯¹è¯å†å²
            tool_call_message = {
                "role": "assistant",
                "content": assistant_message.content,
                "tool_calls": []
            }
            
            # æ ¼å¼åŒ–å·¥å…·è°ƒç”¨ä¿¡æ¯
            for tool_call in assistant_message.tool_calls:
                tool_call_message["tool_calls"].append({
                    "id": tool_call.id,
                    "type": "function",
                    "function": {
                        "name": tool_call.function.name,
                        "arguments": tool_call.function.arguments
                    }
                })
            
            conversation_history.append(tool_call_message)
            
            # æ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶ç®¡ç†å¯¹è¯å†å²
            for tool_call in assistant_message.tool_calls:
                tool_name = tool_call.function.name
                arguments_json = tool_call.function.arguments
                tool_call_id = tool_call.id
                
                # è¾“å‡ºå·¥å…·æ‰§è¡Œå¼€å§‹çŠ¶æ€
                yield f"\nğŸ› ï¸ **æ­£åœ¨è°ƒç”¨å·¥å…·**: `{tool_name}`"
                yield f"\nğŸ“‹ **å‚æ•°**: {self._format_tool_arguments(arguments_json)}"
                logger.info(f"æ‰§è¡Œå·¥å…·è°ƒç”¨: {tool_name}")
                
                try:
                    # æ‰§è¡Œå·¥å…·è°ƒç”¨
                    arguments = json.loads(arguments_json)
                    start_time = time.time()
                    
                    # æ˜¾ç¤ºæ‰§è¡Œä¸­çŠ¶æ€
                    yield f"\nâ³ å·¥å…·æ‰§è¡Œä¸­..."
                    
                    # æ·»åŠ è¶…æ—¶ä¿æŠ¤å’Œè¿æ¥çŠ¶æ€æ£€æŸ¥çš„å·¥å…·è°ƒç”¨
                    try:
                        # æ£€æŸ¥MCPå®¢æˆ·ç«¯çŠ¶æ€
                        if not self.mcp_client:
                            raise Exception("MCPå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
                        
                        # æ£€æŸ¥è¿æ¥çŠ¶æ€
                        if hasattr(self.mcp_client, 'status') and self.mcp_client.status.name != "CONNECTED":
                            logger.warning(f"MCPå®¢æˆ·ç«¯çŠ¶æ€å¼‚å¸¸: {self.mcp_client.status.name}")
                            # å°è¯•é‡æ–°è¿æ¥
                            try:
                                await self.mcp_client.connect()
                            except Exception as reconnect_error:
                                logger.error(f"MCPé‡è¿å¤±è´¥: {reconnect_error}")
                                raise Exception(f"MCPè¿æ¥å¼‚å¸¸ä¸”é‡è¿å¤±è´¥: {reconnect_error}")
                        
                        # æ‰§è¡Œå·¥å…·è°ƒç”¨
                        result = await self.mcp_client.call_tool(tool_name, arguments)
                        execution_time = time.time() - start_time
                        
                        logger.info(f"å·¥å…· {tool_name} æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
                        
                        # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²
                        tool_result_message = {
                            "role": "tool",
                            "tool_call_id": tool_call_id,
                            "content": json.dumps(result, ensure_ascii=False, indent=2)
                        }
                        conversation_history.append(tool_result_message)
                        
                        # è¾“å‡ºå·¥å…·æ‰§è¡Œå®ŒæˆçŠ¶æ€
                        yield f"\nâœ… **å·¥å…·æ‰§è¡ŒæˆåŠŸ**: `{tool_name}` (è€—æ—¶: {execution_time:.2f}ç§’)"
                        yield f"\nğŸ“Š **ç»“æœæ‘˜è¦**: {self._get_result_summary(result)}"
                        
                        # è¿”å›å·¥å…·è°ƒç”¨å®Œæˆä¿¡æ¯
                        yield {
                            "type": "tool_call_complete",
                            "tool_call": {
                                "name": tool_name,
                                "arguments": arguments_json,
                                "id": tool_call_id
                            },
                            "result": result,
                            "conversation_history": conversation_history.copy()
                        }
                        
                    except asyncio.TimeoutError:
                        logger.error(f"å·¥å…·è°ƒç”¨è¶…æ—¶: {tool_name}")
                        execution_time = time.time() - start_time
                        
                        # å°†è¶…æ—¶é”™è¯¯æ·»åŠ åˆ°å¯¹è¯å†å²
                        tool_error_message = {
                            "role": "tool",
                            "tool_call_id": tool_call_id,
                            "content": f"å·¥å…·æ‰§è¡Œè¶…æ—¶: æ‰§è¡Œæ—¶é—´è¶…è¿‡15ç§’"
                        }
                        conversation_history.append(tool_error_message)
                        
                        yield f"â° å·¥å…· {tool_name} æ‰§è¡Œè¶…æ—¶ (è€—æ—¶: {execution_time:.2f}ç§’)"
                        
                        # è¿”å›è¶…æ—¶ä¿¡æ¯
                        yield {
                            "type": "tool_call_complete",
                            "tool_call": {
                                "name": tool_name,
                                "arguments": arguments_json,
                                "id": tool_call_id
                            },
                            "result": None,
                            "error": "æ‰§è¡Œè¶…æ—¶",
                            "conversation_history": conversation_history.copy()
                        }
                    
                    except Exception as tool_error:
                        logger.error(f"å·¥å…·è°ƒç”¨å¼‚å¸¸: {tool_name}: {tool_error}")
                        execution_time = time.time() - start_time
                        
                        # å°†å·¥å…·é”™è¯¯æ·»åŠ åˆ°å¯¹è¯å†å²
                        tool_error_message = {
                            "role": "tool",
                            "tool_call_id": tool_call_id,
                            "content": f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(tool_error)}"
                        }
                        conversation_history.append(tool_error_message)
                        
                        yield f"âŒ å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {str(tool_error)} (è€—æ—¶: {execution_time:.2f}ç§’)"
                        
                        # è¿”å›å·¥å…·è°ƒç”¨å¤±è´¥ä¿¡æ¯
                        yield {
                            "type": "tool_call_complete",
                            "tool_call": {
                                "name": tool_name,
                                "arguments": arguments_json,
                                "id": tool_call_id
                            },
                            "result": None,
                            "error": str(tool_error),
                            "conversation_history": conversation_history.copy()
                        }
                    
                except Exception as e:
                    logger.error(f"å·¥å…·è°ƒç”¨å¤±è´¥ {tool_name}: {e}")
                    
                    # å°†å·¥å…·é”™è¯¯æ·»åŠ åˆ°å¯¹è¯å†å²
                    tool_error_message = {
                        "role": "tool",
                        "tool_call_id": tool_call_id,
                        "content": f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
                    }
                    conversation_history.append(tool_error_message)
                    
                    yield f"âŒ å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {str(e)}"
                    
                    # è¿”å›å·¥å…·è°ƒç”¨å¤±è´¥ä¿¡æ¯
                    yield {
                        "type": "tool_call_complete",
                        "tool_call": {
                            "name": tool_name,
                            "arguments": arguments_json,
                            "id": tool_call_id
                        },
                        "result": None,
                        "error": str(e),
                        "conversation_history": conversation_history.copy()  # æä¾›æ›´æ–°åçš„å¯¹è¯å†å²
                    }
                    
        except Exception as e:
            logger.error(f"ç¬¬ä¸€é˜¶æ®µå·¥å…·æ‰§è¡Œå¤±è´¥: {e}")
            yield f"âŒ å·¥å…·æ‰§è¡Œé˜¶æ®µå¤±è´¥: {str(e)}"

    async def _phase_two_generate_response(
        self, 
        original_message: str, 
        tool_calls: List[Dict[str, Any]], 
        tool_results: List[Any]
    ) -> AsyncGenerator[str, None]:
        """ç¬¬äºŒé˜¶æ®µï¼šåŸºäºå·¥å…·ç»“æœç”ŸæˆLLMå¯¹è¯å›å¤"""
        try:
            logger.info(f"å¼€å§‹ç¬¬äºŒé˜¶æ®µå“åº”ç”Ÿæˆï¼Œå·¥å…·è°ƒç”¨æ•°: {len(tool_calls)}, ç»“æœæ•°: {len(tool_results)}")
            
            # æ„å»ºä¼˜åŒ–çš„æç¤ºè¯
            system_prompt = self._get_tool_response_system_prompt()
            user_prompt = self._get_tool_response_user_prompt(original_message, tool_calls, tool_results)
            
            logger.debug(f"ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)}")
            logger.debug(f"ç”¨æˆ·æç¤ºè¯é•¿åº¦: {len(user_prompt)}")
            
            # æ„å»ºæ¶ˆæ¯å†å²
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            request_params = {
                "model": self._get_model_name(),
                "messages": messages,
                "stream": True
            }
            
            # æ·»åŠ å¯é€‰å‚æ•°
            if self.config.get("temperature") is not None:
                request_params["temperature"] = self.config["temperature"]
            if self.config.get("max_tokens") is not None:
                request_params["max_tokens"] = self.config["max_tokens"]
            
            logger.info(f"ç¬¬äºŒé˜¶æ®µï¼šè°ƒç”¨LLMç”Ÿæˆå›å¤ï¼Œæ¨¡å‹: {self._get_model_name()}")
            
            # æ£€æŸ¥å®¢æˆ·ç«¯çŠ¶æ€
            if not self.client:
                logger.error("LLMå®¢æˆ·ç«¯ä¸ºNoneï¼Œæ— æ³•ç”Ÿæˆå“åº”")
                yield "âŒ LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ç”Ÿæˆå“åº”"
                return
            
            # æµå¼è°ƒç”¨LLMç”Ÿæˆæœ€ç»ˆå›å¤
            try:
                logger.info(f"å‡†å¤‡è°ƒç”¨LLMï¼Œè¯·æ±‚å‚æ•°: {json.dumps(request_params, ensure_ascii=False, indent=2)}")
                stream = await self.client.chat.completions.create(**request_params)
                logger.info("LLMæµå¼è°ƒç”¨å·²å¯åŠ¨")
                
                response_generated = False
                chunk_count = 0
                full_response = ""  # æ”¶é›†å®Œæ•´å“åº”ç”¨äºæœ€ç»ˆæ¢å¤
                
                # åœ¨æµå¼è¾“å‡ºä¸­æ·»åŠ æ¢å¤å¤„ç†
                session_id = getattr(self, 'current_session_id', None)
                if not session_id:
                    logger.error(f"âš ï¸ ä¸¥é‡è­¦å‘Šï¼šæ¢å¤é˜¶æ®µæœªæ‰¾åˆ°ä¼šè¯IDï¼")
                    session_id = f'fallback_session_{int(time.time())}'
                else:
                    logger.error(f"ğŸ†” æ¢å¤é˜¶æ®µä½¿ç”¨ä¼šè¯ID: {session_id}")
                
                async for chunk in stream:
                    chunk_count += 1
                    logger.debug(f"æ”¶åˆ°æµå¼å— #{chunk_count}: {chunk}")
                    if chunk.choices and len(chunk.choices) > 0:
                        delta = chunk.choices[0].delta
                        if delta.content:
                            # æ”¶é›†å®Œæ•´å“åº”ï¼ˆLLMåŸå§‹è¾“å‡ºï¼‰
                            full_response += delta.content
                            response_generated = True
                            
                            # ğŸš€ é˜¶æ®µ1: å®æ—¶æµå¼è¾“å‡ºï¼ˆä¿æŒç”¨æˆ·ä½“éªŒï¼Œå¯èƒ½æœ‰éƒ¨åˆ†è„±æ•å€¼æœªæ¢å¤ï¼‰
                            restored_content = self.data_masker.restore_llm_response(
                                delta.content, session_id
                            )
                            
                            logger.debug(f"æµå¼å— #{chunk_count}: '{delta.content}' â†’ '{restored_content}'")
                            yield restored_content
                        else:
                            logger.debug(f"å— #{chunk_count} æ— å†…å®¹: {delta}")
                    else:
                        logger.debug(f"å— #{chunk_count} æ— choices")
                
                logger.info(f"LLMæµå¼è°ƒç”¨å®Œæˆï¼Œå…±ç”Ÿæˆ {chunk_count} ä¸ªå—ï¼Œæœ‰æ•ˆå“åº”: {response_generated}")
                
                # ğŸ”§ é˜¶æ®µ2: å®Œæ•´å†…å®¹æ¢å¤ï¼ˆä¿®å¤å› chunkåˆ†å‰²å¯¼è‡´çš„æ¢å¤å¤±è´¥ï¼‰
                if response_generated and full_response and session_id:
                    logger.info(f"ğŸ”§ å¼€å§‹å®Œæ•´å“åº”æ¢å¤å¤„ç†...")
                    
                    # å¯¹å®Œæ•´å“åº”è¿›è¡Œè„±æ•æ¢å¤
                    final_restored_response = self.data_masker.restore_llm_response(
                        full_response, session_id
                    )
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„æ¢å¤å†…å®¹
                    if final_restored_response != full_response:
                        logger.info(f"ğŸ¯ æ£€æµ‹åˆ°å®Œæ•´æ¢å¤å·®å¼‚ï¼Œå‘é€æ›´æ–°æŒ‡ä»¤")
                        logger.info(f"ğŸ“ åŸå§‹å®Œæ•´å“åº”é•¿åº¦: {len(full_response)} å­—ç¬¦")
                        logger.info(f"ğŸ”“ æ¢å¤å®Œæ•´å“åº”é•¿åº¦: {len(final_restored_response)} å­—ç¬¦")
                        
                        # å‘é€ç‰¹æ®Šçš„æ›´æ–°æŒ‡ä»¤ï¼Œå‘ŠçŸ¥å®¢æˆ·ç«¯ç”¨æ¢å¤åçš„å®Œæ•´å†…å®¹æ›¿æ¢ä¹‹å‰çš„è¾“å‡º
                        update_instruction = {
                            "type": "content_update", 
                            "content": final_restored_response,
                            "reason": "è„±æ•ä¿¡æ¯æ¢å¤"
                        }
                        
                        yield f"\n\n__UPDATE_CONTENT__:{json.dumps(update_instruction, ensure_ascii=False)}__END_UPDATE__\n"
                        logger.info(f"âœ… å®Œæ•´å†…å®¹æ¢å¤æŒ‡ä»¤å·²å‘é€")
                    else:
                        logger.info(f"ğŸ’­ å®Œæ•´å“åº”æ— éœ€é¢å¤–æ¢å¤")
                
                # å¦‚æœæ²¡æœ‰ç”Ÿæˆä»»ä½•å“åº”ï¼Œæä¾›å›é€€å“åº”
                if not response_generated:
                    logger.warning("LLMæœªç”Ÿæˆä»»ä½•å“åº”ï¼Œä½¿ç”¨å›é€€æœºåˆ¶")
                    async for chunk in self._generate_fallback_response(original_message, tool_calls, tool_results):
                        yield chunk
                        
            except Exception as llm_error:
                logger.error(f"LLMè°ƒç”¨å¼‚å¸¸: {llm_error}", exc_info=True)
                yield f"\nâš ï¸ LLMè°ƒç”¨å¤±è´¥: {str(llm_error)}\n\nä¸ºæ‚¨æä¾›å·¥å…·æ‰§è¡Œç»“æœæ‘˜è¦ï¼š\n\n"
                async for chunk in self._generate_fallback_response(original_message, tool_calls, tool_results):
                    yield chunk
                        
        except Exception as e:
            logger.error(f"ç¬¬äºŒé˜¶æ®µå“åº”ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            # æä¾›å›é€€å“åº”è€Œä¸æ˜¯ç®€å•çš„é”™è¯¯ä¿¡æ¯
            try:
                yield f"\nâš ï¸ å“åº”ç”Ÿæˆé‡åˆ°é—®é¢˜ï¼Œä¸ºæ‚¨æä¾›å·¥å…·æ‰§è¡Œç»“æœæ‘˜è¦ï¼š\n\n"
                async for chunk in self._generate_fallback_response(original_message, tool_calls, tool_results):
                    yield chunk
            except Exception as fallback_error:
                logger.error(f"å›é€€å“åº”ç”Ÿæˆä¹Ÿå¤±è´¥: {fallback_error}")
                yield f"\nâŒ å“åº”ç”Ÿæˆå¤±è´¥: {str(e)}"

    async def _stream_llm_response(self, conversation_history: List[Dict[str, Any]]) -> AsyncGenerator[str, None]:
        """æµå¼è¾“å‡ºLLMå“åº”ï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰"""
        try:
            # æ„å»ºè¯·æ±‚å‚æ•°
            request_params = {
                "model": self._get_model_name(),
                "messages": conversation_history,
                "stream": True
            }
            
            # æ·»åŠ å¯é€‰å‚æ•°
            if self.config.get("temperature") is not None:
                request_params["temperature"] = self.config["temperature"]
            if self.config.get("max_tokens") is not None:
                request_params["max_tokens"] = self.config["max_tokens"]
            
            logger.info("æµå¼è¾“å‡ºæ™®é€šLLMå“åº”")
            
            # æµå¼è°ƒç”¨LLM
            stream = await self.client.chat.completions.create(**request_params)
            
            async for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if delta.content:
                        yield delta.content
                        
        except Exception as e:
            logger.error(f"æµå¼å“åº”å¤±è´¥: {e}")
            yield f"âŒ å“åº”ç”Ÿæˆå¤±è´¥: {str(e)}"

    def _get_tool_response_system_prompt(self) -> str:
        """è·å–å·¥å…·ç»“æœè§£è¯»çš„ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ï¼Œéœ€è¦åŸºäºå·¥å…·æ‰§è¡Œç»“æœä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›å¤ã€‚

é‡è¦åŸåˆ™ï¼š
1. ä»¥å·¥å…·æ‰§è¡Œç»“æœä¸ºå‡† - ä½ çš„å›å¤å¿…é¡»åŸºäºå®é™…çš„å·¥å…·æ‰§è¡Œç»“æœï¼Œä¸è¦æ·»åŠ æœªç»éªŒè¯çš„ä¿¡æ¯
2. å‡†ç¡®è§£è¯»ç»“æœ - ä»”ç»†åˆ†æå·¥å…·è¿”å›çš„æ•°æ®ï¼Œå‡†ç¡®ç†è§£å…¶å«ä¹‰
3. ç»“æ„åŒ–å‘ˆç° - å°†å·¥å…·ç»“æœä»¥æ¸…æ™°ã€æ˜“è¯»çš„æ–¹å¼å‘ˆç°ç»™ç”¨æˆ·
4. çªå‡ºå…³é”®ä¿¡æ¯ - è¯†åˆ«å¹¶å¼ºè°ƒå·¥å…·ç»“æœä¸­çš„é‡è¦ä¿¡æ¯
5. æä¾›ä¸Šä¸‹æ–‡ - è§£é‡Šå·¥å…·ç»“æœçš„æ„ä¹‰å’Œç›¸å…³æ€§
6. é¿å…æ¨æµ‹ - ä¸è¦åŸºäºå·¥å…·ç»“æœè¿›è¡Œè¿‡åº¦æ¨æµ‹æˆ–æ·»åŠ ä¸ç¡®å®šçš„ä¿¡æ¯

å¦‚æœå·¥å…·æ‰§è¡Œå¤±è´¥æˆ–è¿”å›é”™è¯¯ï¼Œè¯·å¦‚å®è¯´æ˜æƒ…å†µå¹¶æä¾›å¯èƒ½çš„è§£å†³å»ºè®®ã€‚"""

    def _get_tool_response_user_prompt(
        self, 
        original_question: str, 
        tool_calls: List[Dict[str, Any]], 
        tool_results: List[Any]
    ) -> str:
        """è·å–å·¥å…·ç»“æœè§£è¯»çš„ç”¨æˆ·æç¤ºè¯ - æ·»åŠ è„±æ•å¤„ç†"""
        
        # ğŸ”’ å…³é”®è„±æ•ç‚¹ï¼šåœ¨åºåˆ—åŒ–å‰å¯¹å·¥å…·ç»“æœè¿›è¡Œè„±æ•
        # ç”Ÿæˆå”¯ä¸€çš„ä¼šè¯IDï¼Œç¡®ä¿è„±æ•å’Œæ¢å¤ä½¿ç”¨åŒä¸€ä¸ªID
        import time
        session_id = f'session_{int(time.time() * 1000)}_{hash(str(tool_results))}'
        self.current_session_id = session_id  # ç¡®ä¿ä¿å­˜ä¼šè¯ID
        
        logger.error(f"ğŸ†” ä¼šè¯IDç”Ÿæˆ: {session_id}")
        
        masked_tool_results = self.data_masker.mask_tool_results(tool_results, session_id)
        
        # ğŸ“ åœ¨æ—¥å¿—ä¸­è®°å½•è¯¦ç»†çš„è„±æ•æ•ˆæœ
        import json
        logger.error(f"ğŸ”’ğŸ”’ğŸ”’ è„±æ•å¤„ç†è¯¦ç»†æ—¥å¿— (ä¼šè¯ID: {session_id}) ğŸ”’ğŸ”’ğŸ”’")
        logger.error(f"ğŸ“‹ åŸå§‹å·¥å…·ç»“æœ:")
        for i, result in enumerate(tool_results, 1):
            result_json = json.dumps(result, ensure_ascii=False)
            logger.error(f"   å·¥å…·ç»“æœ#{i}: {result_json[:500]}...")
        
        logger.error(f"ğŸ”’ è„±æ•åå·¥å…·ç»“æœ:")
        for i, result in enumerate(masked_tool_results, 1):
            result_json = json.dumps(result, ensure_ascii=False)
            logger.error(f"   è„±æ•ç»“æœ#{i}: {result_json[:500]}...")
        
        # è·å–æ˜ å°„ç»Ÿè®¡
        stats = self.data_masker.get_session_stats(session_id)
        logger.error(f"ğŸ“Š è„±æ•æ˜ å°„ç»Ÿè®¡: {stats['mapping_count']} ä¸ªæ˜ å°„å…³ç³»")
        
        # æ˜¾ç¤ºå…·ä½“çš„æ˜ å°„å…³ç³»ï¼ˆè°ƒè¯•ç”¨ï¼‰
        mapping_store = self.data_masker.session_manager.get_session(session_id)
        if mapping_store:
            logger.error(f"ğŸ—„ï¸ æ˜ å°„å…³ç³»è¯¦æƒ…:")
            for original, masked in list(mapping_store.original_to_masked.items())[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                logger.error(f"   '{original}' â†’ '{masked}'")
        
        logger.error(f"ğŸ”’ğŸ”’ğŸ”’ è„±æ•å¤„ç†æ—¥å¿—ç»“æŸ ğŸ”’ğŸ”’ğŸ”’")
        
        # æ ¼å¼åŒ–å·¥å…·æ‰§è¡Œç»“æœï¼ˆä½¿ç”¨è„±æ•åçš„æ•°æ®ï¼‰
        formatted_results = []
        for i, (tool_call, result) in enumerate(zip(tool_calls, masked_tool_results), 1):
            tool_name = tool_call.get("name", "æœªçŸ¥å·¥å…·")
            if result is not None:
                if hasattr(result, '__dict__'):
                    result_str = json.dumps(result.__dict__, ensure_ascii=False, indent=2)
                else:
                    result_str = json.dumps(result, ensure_ascii=False, indent=2)
                formatted_results.append(f"{i}. å·¥å…·: {tool_name}\n   ç»“æœ: {result_str}")
            else:
                error = tool_call.get("error", "æ‰§è¡Œå¤±è´¥")
                formatted_results.append(f"{i}. å·¥å…·: {tool_name}\n   é”™è¯¯: {error}")
        
        results_text = "\n\n".join(formatted_results)
        
        return f"""ç”¨æˆ·é—®é¢˜ï¼š{original_question}

å·¥å…·æ‰§è¡Œç»“æœï¼š
{results_text}

è¯·åŸºäºå·¥å…·æ‰§è¡Œç»“æœå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¦æ±‚ï¼š
1. ç›´æ¥å›ç­”é—®é¢˜
2. åŸºäºå®é™…ç»“æœ
3. æ ¼å¼æ¸…æ™°æ˜“è¯»ï¼Œä½¿ç”¨æ¢è¡Œç¬¦åˆ†éš”ä¸åŒéƒ¨åˆ†
4. å¯¹äºè¡¨æ ¼æ•°æ®ï¼Œè¯·ä½¿ç”¨æ ‡å‡†çš„Markdownè¡¨æ ¼æ ¼å¼
5. åœ¨ä¸åŒä¿¡æ¯å—ä¹‹é—´æ·»åŠ ç©ºè¡Œ

è¯·å¼€å§‹å›å¤ï¼š"""

    def _format_tool_arguments(self, arguments_json: str) -> str:
        """æ ¼å¼åŒ–å·¥å…·å‚æ•°ä¸ºç”¨æˆ·å‹å¥½çš„æ˜¾ç¤º"""
        try:
            arguments = json.loads(arguments_json)
            if isinstance(arguments, dict):
                # æ ¼å¼åŒ–ä¸ºç®€æ´çš„å‚æ•°æ˜¾ç¤º
                formatted_args = []
                for key, value in arguments.items():
                    if isinstance(value, str) and len(value) > 50:
                        # é•¿å­—ç¬¦ä¸²æˆªæ–­æ˜¾ç¤º
                        formatted_args.append(f"{key}={value[:47]}...")
                    else:
                        formatted_args.append(f"{key}={value}")
                return ", ".join(formatted_args)
            else:
                return str(arguments)
        except json.JSONDecodeError:
            return arguments_json[:100] + "..." if len(arguments_json) > 100 else arguments_json

    async def _generate_fallback_response(
        self, 
        original_message: str, 
        tool_calls: List[Dict[str, Any]], 
        tool_results: List[Any]
    ) -> AsyncGenerator[str, None]:
        """ç”Ÿæˆå›é€€å“åº”ï¼Œå½“LLMæ— æ³•ç”Ÿæˆå“åº”æ—¶ä½¿ç”¨"""
        try:
            yield "åŸºäºå·¥å…·æ‰§è¡Œç»“æœï¼Œä¸ºæ‚¨æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š\n\n"
            
            for i, (tool_call, result) in enumerate(zip(tool_calls, tool_results), 1):
                tool_name = tool_call.get("name", "æœªçŸ¥å·¥å…·")
                yield f"**{i}. å·¥å…·: {tool_name}**\n"
                
                if result is not None:
                    # æ ¼å¼åŒ–å·¥å…·ç»“æœ
                    if isinstance(result, dict):
                        if "items" in result and isinstance(result["items"], list):
                            items = result["items"]
                            yield f"æ‰¾åˆ° {len(items)} ä¸ªèµ„æº\n"
                            if items:
                                yield "ä¸»è¦èµ„æºï¼š\n"
                                for item in items[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                                    # å®‰å…¨åœ°å¤„ç†å¯èƒ½ä¸ºNoneçš„item
                                    if item is not None and isinstance(item, dict):
                                        metadata = item.get("metadata", {})
                                        if isinstance(metadata, dict):
                                            yield f"- {metadata.get('name', 'Unknown')}\n"
                                        else:
                                            yield f"- {item.get('name', 'Unknown')}\n"
                                    else:
                                        yield f"- æ— æ•ˆèµ„æºé¡¹\n"
                        elif "content" in result:
                            content = result["content"]
                            if len(content) > 200:
                                yield f"æ—¥å¿—å†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰ï¼š\n{content[:200]}...\n"
                            else:
                                yield f"æ—¥å¿—å†…å®¹ï¼š\n{content}\n"
                        else:
                            # é€šç”¨å­—å…¸ç»“æœ
                            try:
                                yield f"æ‰§è¡Œç»“æœï¼š{json.dumps(result, ensure_ascii=False, indent=2)[:300]}...\n"
                            except Exception as json_error:
                                yield f"æ‰§è¡Œç»“æœï¼š{str(result)[:300]}...\n"
                    elif isinstance(result, list):
                        yield f"è¿”å›åˆ—è¡¨ï¼ŒåŒ…å« {len(result)} é¡¹\n"
                    else:
                        yield f"ç»“æœï¼š{str(result)[:200]}...\n"
                else:
                    error = tool_call.get("error", "æ‰§è¡Œå¤±è´¥")
                    yield f"âŒ æ‰§è¡Œå¤±è´¥ï¼š{error}\n"
                
                yield "\n"
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›é€€å“åº”å¤±è´¥: {e}")
            yield f"å·¥å…·æ‰§è¡Œå®Œæˆï¼Œä½†æ— æ³•ç”Ÿæˆè¯¦ç»†è¯´æ˜ã€‚"

    def _get_result_summary(self, result: Any) -> str:
        """è·å–å·¥å…·ç»“æœçš„ç®€è¦æ‘˜è¦"""
        try:
            if isinstance(result, dict):
                if "items" in result and isinstance(result["items"], list):
                    # K8s èµ„æºåˆ—è¡¨
                    items = result["items"]
                    count = len(items)
                    return f"æ‰¾åˆ° {count} ä¸ªèµ„æº"
                elif "pod_name" in result:
                    # Pod æ—¥å¿—
                    return f"è·å–åˆ° Pod {result['pod_name']} çš„æ—¥å¿—"
                elif "deployment_name" in result:
                    # éƒ¨ç½²æ“ä½œ
                    return f"éƒ¨ç½² {result['deployment_name']} æ“ä½œå®Œæˆ"
                elif "success" in result:
                    # é€šç”¨æˆåŠŸ/å¤±è´¥ç»“æœ
                    return "æ“ä½œæˆåŠŸ" if result["success"] else "æ“ä½œå¤±è´¥"
                else:
                    # é€šç”¨å­—å…¸ç»“æœ
                    try:
                        keys = list(result.keys())[:3]  # æ˜¾ç¤ºå‰3ä¸ªé”®
                        return f"è¿”å›æ•°æ®åŒ…å«: {', '.join(keys)}"
                    except Exception:
                        return "è¿”å›å­—å…¸æ•°æ®"
            elif isinstance(result, list):
                return f"è¿”å›åˆ—è¡¨ï¼ŒåŒ…å« {len(result)} é¡¹"
            elif isinstance(result, str):
                return f"è¿”å›æ–‡æœ¬ï¼Œé•¿åº¦ {len(result)} å­—ç¬¦"
            else:
                return f"è¿”å› {type(result).__name__} ç±»å‹æ•°æ®"
        except Exception as e:
            logger.warning(f"è·å–ç»“æœæ‘˜è¦æ—¶å‡ºé”™: {e}")
            return "ç»“æœå¤„ç†å®Œæˆ"

    async def _execute_tool_call(self, tool_name: str, arguments_json: str) -> str:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶è¿”å›æ ¼å¼åŒ–ç»“æœ"""
        try:
            if not self.mcp_client:
                logger.warning("MCPå®¢æˆ·ç«¯æœªè¿æ¥ï¼Œæ— æ³•æ‰§è¡Œå·¥å…·è°ƒç”¨")
                return "âŒ MCPå®¢æˆ·ç«¯æœªè¿æ¥ï¼Œæ— æ³•æ‰§è¡Œå·¥å…·è°ƒç”¨"
            
            logger.info(f"æ‰§è¡Œå·¥å…·è°ƒç”¨: {tool_name}")
            logger.debug(f"å·¥å…·å‚æ•°: {arguments_json}")
            
            import json
            try:
                arguments = json.loads(arguments_json)
            except json.JSONDecodeError as e:
                logger.error(f"å·¥å…·å‚æ•°JSONè§£æå¤±è´¥: {e}")
                return f"âŒ å·¥å…·å‚æ•°æ ¼å¼é”™è¯¯: {str(e)}"
            
            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            start_time = time.time()
            result = await self.mcp_client.call_tool(tool_name, arguments)
            execution_time = time.time() - start_time
            
            logger.info(f"å·¥å…· {tool_name} æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_result = self.format_tool_result(result)
            return formatted_result
            
        except Exception as e:
            logger.error(f"å·¥å…·è°ƒç”¨å¤±è´¥ {tool_name}: {e}", exc_info=True)
            return f"âŒ å·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}"
    
    async def _get_available_tools(self):
        """è·å–å¯ç”¨å·¥å…·åˆ—è¡¨"""
        try:
            if not self.mcp_client:
                return None
            
            available_tools = await self.mcp_client.list_tools()
            if available_tools:
                return self._convert_tools_to_openai(available_tools)
            return None
            
        except Exception as e:
            logger.warning(f"è·å–å·¥å…·åˆ—è¡¨å¤±è´¥: {e}")
            return None


    

    
    async def chat(
        self, 
        messages: List[ChatMessage], 
        user_id: str = "default",
        stream: bool = False,
        provider_id: Optional[str] = None
    ) -> Union[AsyncGenerator[str, None], ProcessResult]:
        """
        èŠå¤©æ–¹æ³• - æ”¯æŒä¾›åº”å•†åˆ‡æ¢
        
        :param messages: æ¶ˆæ¯åˆ—è¡¨
        :param user_id: ç”¨æˆ·ID
        :param stream: æ˜¯å¦æµå¼è¾“å‡º
        :param provider_id: æŒ‡å®šä¾›åº”å•†IDï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰ä¾›åº”å•†
        """
        # å¦‚æœæŒ‡å®šäº†ä¾›åº”å•†ï¼Œä¸´æ—¶åˆ‡æ¢
        original_provider = self.config.get("provider", "openai")
        if provider_id and provider_id != original_provider:
            logger.warning(f"provider_id å‚æ•°å·²åºŸå¼ƒï¼Œå½“å‰é…ç½®ä¸º {original_provider}ï¼Œæ— æ³•åˆ‡æ¢ã€‚")
        
        try:
            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨
            if not self.config.get("enabled", True):
                result = "LLMåŠŸèƒ½å·²ç¦ç”¨ï¼Œæ— æ³•è¿›è¡ŒèŠå¤©ã€‚è¯·åœ¨è®¾ç½®ä¸­å¯ç”¨LLMåŠŸèƒ½ã€‚"
                if stream:
                    async def _yield_disabled():
                        yield result
                    return _yield_disabled()
                else:
                    return ProcessResult(content=result)
            
            # æ£€æŸ¥å½“å‰ä¾›åº”å•†æ˜¯å¦å¯ç”¨
            current_provider = self.get_current_provider()
            if not current_provider or not current_provider.get("enabled", True):
                result = f"å½“å‰ä¾›åº”å•†ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚"
                if stream:
                    async def _yield_unavailable():
                        yield result
                    return _yield_unavailable()
                else:
                    return ProcessResult(content=result)
            
            # ä½¿ç”¨æµå¼æˆ–éæµå¼æ–¹å¼
            if stream:
                # æµå¼è¾“å‡º
                return self._stream_chat_response(messages, user_id)
            else:
                # éæµå¼è¾“å‡º
                return await self._process_chat(messages, user_id)
                
        finally:
            # æ¢å¤åŸä¾›åº”å•†
            if provider_id and provider_id != original_provider:
                logger.warning(f"provider_id å‚æ•°å·²åºŸå¼ƒï¼Œæ— æ³•åˆ‡æ¢ä¾›åº”å•†ã€‚å½“å‰é…ç½®ä¸º {original_provider}ã€‚")
    
    async def chat_with_shortcuts(
        self,
        shortcut: str,
        content: str,
        context: Optional[Dict[str, Any]] = None
    ) -> ProcessResult:
        """å¿«æ·æŒ‡ä»¤å¤„ç†"""
        shortcut_prompts = {
            "/pods": "è¯·è·å–Kubernetesé›†ç¾¤ä¸­çš„Podåˆ—è¡¨ï¼Œå¹¶ä»¥æ˜“è¯»çš„æ ¼å¼å±•ç¤º",
            "/logs": "è¯·è·å–æŒ‡å®šPodçš„æœ€æ–°æ—¥å¿—",
            "/scale": "è¯·æ‰©ç¼©å®¹æŒ‡å®šçš„Deployment",
            "/status": "è¯·æ£€æŸ¥é›†ç¾¤çŠ¶æ€å’Œå¥åº·æƒ…å†µ",
            "/help": "æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨çš„å¿«æ·æŒ‡ä»¤"
        }
        
        prompt = shortcut_prompts.get(shortcut)
        if not prompt:
            return ProcessResult(
                content=f"æœªçŸ¥çš„å¿«æ·æŒ‡ä»¤: {shortcut}\n\nå¯ç”¨æŒ‡ä»¤:\n" + 
                       "\n".join(f"- {k}: {v}" for k, v in shortcut_prompts.items())
            )
        
        # æ„å»ºæ¶ˆæ¯
        messages = [
            ChatMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Kubernetesè¿ç»´åŠ©æ‰‹ï¼Œæ“…é•¿ä½¿ç”¨K8så·¥å…·æ¥ç®¡ç†é›†ç¾¤ã€‚"),
            ChatMessage(role="user", content=f"{prompt}\n\nç”¨æˆ·è¡¥å……ä¿¡æ¯: {content}")
        ]
        
        return await self.chat(messages, enable_tools=True)
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=60))
    async def _chat_without_tools(self, messages: List[ChatMessage]) -> ProcessResult:
        """ä¸ä½¿ç”¨å·¥å…·çš„èŠå¤©"""
        if self.client is None:
            return ProcessResult(
                content="LLMå®¢æˆ·ç«¯æœªæ­£ç¡®åˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚"
            )
            
        openai_messages = self._convert_messages_to_openai(messages)
        
        try:
            # ä½¿ç”¨å¼‚æ­¥createæ–¹æ³•
            response = await self.client.chat.completions.create(
                model=self._get_model_name(),
                messages=openai_messages,
                temperature=self.config.get("temperature", 0.7),
                max_tokens=self.config.get("max_tokens", 2000)
            )
        except Exception as e:
            logger.error(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
            return ProcessResult(
                content=f"è°ƒç”¨LLMæœåŠ¡å¤±è´¥: {str(e)}"
            )
        
        # ç¡®ä¿å“åº”å†…å®¹ä¸ä¸ºç©º
        content = ""
        if response and response.choices and len(response.choices) > 0:
            message = response.choices[0].message
            content = message.content or ""
        
        # å¦‚æœå†…å®¹ä¸ºç©ºï¼Œæä¾›é»˜è®¤å“åº”
        if not content:
            content = "æŠ±æ­‰ï¼Œæˆ‘æ²¡èƒ½ç”Ÿæˆæœ‰æ•ˆçš„å“åº”ï¼Œè¯·é‡è¯•ã€‚"
        
        # è§„èŒƒåŒ–usageä¿¡æ¯
        usage_dict = self._simplify_usage(getattr(response, 'usage', None))
        
        return ProcessResult(
            content=content,
            usage=usage_dict
        )
    
    async def _chat_with_tools(self, messages: List[ChatMessage]) -> ProcessResult:
        """ä½¿ç”¨å·¥å…·çš„èŠå¤©"""
        if self.client is None:
            return ProcessResult(
                content="LLMå®¢æˆ·ç«¯æœªæ­£ç¡®åˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚"
            )
            
        # è·å–å¯ç”¨å·¥å…·
        try:
            if not self.mcp_client:
                return await self._chat_without_tools(messages)
            tools = await self.mcp_client.list_tools()
        except Exception as e:
            logger.warning(f"è·å–MCPå·¥å…·å¤±è´¥: {e}")
            return await self._chat_without_tools(messages)
            
        if not tools:
            return await self._chat_without_tools(messages)
        
        # è½¬æ¢å·¥å…·ä¸º OpenAI æ ¼å¼
        openai_tools = self._convert_tools_to_openai(tools)
        openai_messages = self._convert_messages_to_openai(messages)
        
        # è°ƒç”¨ LLM
        try:
            # ä½¿ç”¨å¼‚æ­¥createæ–¹æ³•
            response = await self.client.chat.completions.create(
                model=self._get_model_name(),
                messages=openai_messages,
                temperature=self.config.get("temperature", 0.7),
                max_tokens=self.config.get("max_tokens", 2000),
                tools=openai_tools,
                tool_choice="auto"
            )
        except Exception as e:
            logger.error(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
            return ProcessResult(
                content=f"è°ƒç”¨LLMæœåŠ¡å¤±è´¥: {str(e)}"
            )
        
        # å®‰å…¨åœ°è®¿é—®response.choices
        if not response or not response.choices or len(response.choices) == 0:
            logger.error("LLMå“åº”å¼‚å¸¸ï¼šchoicesä¸ºç©º")
            return ProcessResult(
                content="LLMå“åº”å¼‚å¸¸ï¼Œè¯·é‡è¯•ã€‚"
            )
            
        message = response.choices[0].message
        
        # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›
        if not message.tool_calls:
            # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
            content = message.content or "æŠ±æ­‰ï¼Œæˆ‘æ²¡èƒ½ç”Ÿæˆæœ‰æ•ˆçš„å“åº”ï¼Œè¯·é‡è¯•ã€‚"
            
            # è§„èŒƒåŒ–usageä¿¡æ¯
            usage_dict = self._simplify_usage(getattr(response, 'usage', None))
            
            return ProcessResult(
                content=content,
                usage=usage_dict
            )
        
        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        function_results = []
        for tool_call in message.tool_calls:
            try:
                # è§£æå‚æ•°
                parameters = json.loads(tool_call.function.arguments)
                
                # è°ƒç”¨ MCP å·¥å…·
                if not self.mcp_client:
                    result = "MCPå®¢æˆ·ç«¯æœªè¿æ¥ï¼Œæ— æ³•æ‰§è¡Œå·¥å…·è°ƒç”¨"
                else:
                    result = await self.mcp_client.call_tool(
                    tool_call.function.name,
                    parameters
                )
                
                function_results.append(FunctionCallResult(
                    function_call=FunctionCall(
                        name=tool_call.function.name,
                        arguments=tool_call.function.arguments
                    ),
                    result=result
                ))
                
                # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
                openai_messages.append({
                    "role": "assistant",
                    "content": message.content,
                    "tool_calls": [{
                        "id": tool_call.id,
                        "type": "function",
                        "function": {
                            "name": tool_call.function.name,
                            "arguments": tool_call.function.arguments
                        }
                    }]
                })
                
                # ä½¿ç”¨æ™ºèƒ½ç»“æœå¤„ç†
                processed_content = self._process_mcp_result(result, tool_call.function.name)
                
                openai_messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": processed_content
                })
                
            except Exception as e:
                logger.error(f"å·¥å…·è°ƒç”¨å¤±è´¥ {tool_call.function.name}: {e}")
                function_results.append(FunctionCallResult(
                    function_call=FunctionCall(
                        name=tool_call.function.name,
                        arguments=tool_call.function.arguments
                    ),
                    error=str(e)
                ))
        
        # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ç»“æœï¼Œå†æ¬¡è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆå›å¤
        if function_results:
            try:
                # ä¼˜åŒ–ä¸Šä¸‹æ–‡å¤§å°
                optimized_messages = self._optimize_context_size(openai_messages)
                
                # ä½¿ç”¨å¼‚æ­¥createæ–¹æ³•
                final_response = await self.client.chat.completions.create(
                    model=self._get_model_name(),
                    messages=optimized_messages,
                    temperature=self.config.get("temperature", 0.7),
                    max_tokens=self.config.get("max_tokens", 2000)
                )
            except Exception as e:
                logger.error(f"æœ€ç»ˆå“åº”ç”Ÿæˆå¤±è´¥: {e}")
                return ProcessResult(
                    content="å·¥å…·è°ƒç”¨å®Œæˆï¼Œä½†ç”Ÿæˆæœ€ç»ˆå“åº”æ—¶å‡ºé”™ã€‚",
                    function_calls=function_results
                )
            
            # ç¡®ä¿æœ€ç»ˆå†…å®¹ä¸ä¸ºç©º
            final_message_content = ""
            if final_response and final_response.choices and len(final_response.choices) > 0:
                final_message_content = final_response.choices[0].message.content or ""
            
            final_content = self._format_response_with_tools(
                final_message_content,
                function_results
            )
            
            # è§„èŒƒåŒ–usageä¿¡æ¯
            usage_dict = self._simplify_usage(getattr(final_response, 'usage', None))
            
            return ProcessResult(
                content=final_content,
                function_calls=function_results,
                usage=usage_dict
            )
        
        # æœ€ç»ˆå›é€€å¤„ç†
        content = message.content or "å·¥å…·è°ƒç”¨å®Œæˆ"
        
        # è§„èŒƒåŒ–usageä¿¡æ¯
        usage_dict = self._simplify_usage(getattr(response, 'usage', None))
        
        return ProcessResult(
            content=content,
            function_calls=function_results,
            usage=usage_dict
        )
    
    def _convert_messages_to_openai(self, messages: List[ChatMessage]) -> List[Dict[str, Any]]:
        """è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸º OpenAI æ ¼å¼"""
        result = []
        for msg in messages:
            openai_msg = {
                "role": msg.role,
                "content": msg.content
            }
            
            if msg.tool_call_id:
                openai_msg["tool_call_id"] = msg.tool_call_id
                
            if msg.function_call:
                openai_msg["function_call"] = {
                    "name": msg.function_call.name,
                    "arguments": msg.function_call.arguments
                }
            
            result.append(openai_msg)
        
        return result
    
    def _convert_tools_to_openai(self, tools) -> List[Dict[str, Any]]:
        """è½¬æ¢ MCP å·¥å…·ä¸º OpenAI å·¥å…·æ ¼å¼"""
        result = []
        for i, tool in enumerate(tools):
            try:
                logger.debug(f"ğŸ”§ å¤„ç†å·¥å…· {i+1}/{len(tools)}: {tool.name}")
                
                # æ¸…ç†schemaä»¥ç¡®ä¿ä¸ä¸åŒAPIæä¾›å•†çš„å…¼å®¹æ€§
                cleaned_schema = self._clean_schema_for_compatibility(tool.input_schema)
                
                converted_tool = {
                    "type": "function",
                    "function": {
                        "name": tool.name,
                        "description": tool.description,
                        "parameters": cleaned_schema
                    }
                }
                result.append(converted_tool)
                logger.debug(f"âœ… å·¥å…· {tool.name} è½¬æ¢æˆåŠŸ")
                
            except Exception as e:
                logger.error(f"âŒ å·¥å…· {tool.name} è½¬æ¢å¤±è´¥: {e}")
                # ç»§ç»­å¤„ç†å…¶ä»–å·¥å…·ï¼Œä¸å› å•ä¸ªå·¥å…·å¤±è´¥è€Œåœæ­¢
                continue
                
        return result
    
    def _clean_schema_for_compatibility(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…ç†JSON schemaä»¥ç¡®ä¿APIå…¼å®¹æ€§"""
        if not isinstance(schema, dict):
            return schema
        
        try:
            cleaned = {}
            for key, value in schema.items():
                if key == "properties" and isinstance(value, dict):
                    # æ¸…ç†propertiesä¸­çš„ç±»å‹å®šä¹‰
                    logger.debug(f"ğŸ”§ æ¸…ç†schema propertiesï¼ŒåŒ…å« {len(value)} ä¸ªå±æ€§")
                    cleaned[key] = {}
                    for prop_name, prop_def in value.items():
                        logger.debug(f"ğŸ”§ æ¸…ç†å±æ€§: {prop_name}")
                        cleaned[key][prop_name] = self._clean_property_definition(prop_def, 0)
                else:
                    cleaned[key] = value
            
            return cleaned
        except Exception as e:
            logger.error(f"âŒ Schemaæ¸…ç†å¤±è´¥: {e}")
            # è¿”å›åŸå§‹schemaä½œä¸ºfallback
            return schema
    
    def _clean_property_definition(self, prop_def: Dict[str, Any], _depth: int = 0) -> Dict[str, Any]:
        """æ¸…ç†å•ä¸ªå±æ€§å®šä¹‰"""
        if not isinstance(prop_def, dict):
            return prop_def
        
        # é˜²æ­¢é€’å½’è¿‡æ·±å¯¼è‡´æ ˆæº¢å‡º
        if _depth > 10:
            logger.warning(f"ğŸ”§ Schemaé€’å½’æ·±åº¦è¶…è¿‡10å±‚ï¼Œåœæ­¢å¤„ç†")
            return prop_def
        
        cleaned = prop_def.copy()
        
        # å¤„ç†ç±»å‹å®šä¹‰
        if "type" in cleaned:
            type_value = cleaned["type"]
            if isinstance(type_value, list):
                # å°†æ•°ç»„ç±»å‹è½¬æ¢ä¸ºå•ä¸€ç±»å‹ï¼ˆé€‰æ‹©ç¬¬ä¸€ä¸ªï¼Œé€šå¸¸æ˜¯ä¸»è¦ç±»å‹ï¼‰
                if type_value:
                    old_type = type_value
                    cleaned["type"] = type_value[0]
                    logger.debug(f"ğŸ”§ è½¬æ¢æ•°ç»„ç±»å‹ {old_type} ä¸º {type_value[0]}")
                else:
                    cleaned["type"] = "string"  # é»˜è®¤ä¸ºstring
        
        # é€’å½’å¤„ç†åµŒå¥—çš„schema
        for key, value in cleaned.items():
            if key == "items" and isinstance(value, dict):
                cleaned[key] = self._clean_property_definition(value, _depth + 1)
            elif key == "properties" and isinstance(value, dict):
                cleaned[key] = {}
                for sub_key, sub_value in value.items():
                    cleaned[key][sub_key] = self._clean_property_definition(sub_value, _depth + 1)
        
        return cleaned
    
    def _format_response_with_tools(
        self,
        content: str,
        function_results: List[FunctionCallResult]
    ) -> str:
        """æ ¼å¼åŒ–åŒ…å«å·¥å…·è°ƒç”¨ç»“æœçš„å“åº”"""
        if not function_results:
            return content
        
        formatted_content = content + "\n\n**å·¥å…·è°ƒç”¨è¯¦æƒ…:**\n"
        
        for i, result in enumerate(function_results, 1):
            formatted_content += f"\n**{i}. {result.function_call.name}**\n"
            
            if result.error:
                formatted_content += f"âŒ æ‰§è¡Œå¤±è´¥: {result.error}\n"
            else:
                formatted_content += f"âœ… æ‰§è¡ŒæˆåŠŸ\n"
                # ç®€åŒ–ç»“æœæ˜¾ç¤º
                try:
                    if isinstance(result.result, dict):
                        if "items" in result.result and isinstance(result.result.get("items"), list):
                            items = result.result["items"]
                            formatted_content += f"ğŸ“Š è¿”å› {len(items)} é¡¹ç»“æœ\n"
                        else:
                            result_str = json.dumps(result.result, ensure_ascii=False, indent=2)[:200]
                            formatted_content += f"ğŸ“‹ ç»“æœ: {result_str}...\n"
                    else:
                        formatted_content += f"ğŸ“‹ ç»“æœ: {str(result.result)[:200]}...\n"
                except Exception as format_error:
                    logger.warning(f"æ ¼å¼åŒ–å·¥å…·ç»“æœæ—¶å‡ºé”™: {format_error}")
                    formatted_content += f"ğŸ“‹ ç»“æœ: æ•°æ®æ ¼å¼åŒ–å¤±è´¥\n"
        
        return formatted_content
    
    async def get_available_shortcuts(self) -> Dict[str, str]:
        """è·å–å¯ç”¨çš„å¿«æ·æŒ‡ä»¤"""
        shortcuts = {
            "/pods": "æŸ¥çœ‹Podåˆ—è¡¨",
            "/logs": "æŸ¥çœ‹Podæ—¥å¿—",
            "/scale": "æ‰©ç¼©å®¹Deployment", 
            "/status": "æ£€æŸ¥é›†ç¾¤çŠ¶æ€",
            "/help": "æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"
        }
        
        # å¦‚æœ MCP å®¢æˆ·ç«¯è¿æ¥ï¼Œæ·»åŠ å·¥å…·ç›¸å…³çš„å¿«æ·æŒ‡ä»¤
        if self.mcp_client and hasattr(self.mcp_client, 'status') and self.mcp_client.status.value == "connected":
            try:
                tools = await self.mcp_client.list_tools()
                for tool in tools:
                    shortcuts[f"/tool-{tool.name}"] = f"ç›´æ¥è°ƒç”¨å·¥å…·: {tool.description}"
            except Exception as e:
                logger.warning(f"è·å–å·¥å…·å¿«æ·æŒ‡ä»¤å¤±è´¥: {e}")
        
        return shortcuts
    
    def format_tool_result(self, result: Any) -> str:
        """æ ¼å¼åŒ–å·¥å…·è°ƒç”¨ç»“æœä¸ºç”¨æˆ·å‹å¥½çš„æ–‡æœ¬"""
        try:
            if isinstance(result, dict):
                if "items" in result and isinstance(result["items"], list):
                    # K8s èµ„æºåˆ—è¡¨æ ¼å¼
                    items = result["items"]
                    if not items:
                        return "ğŸ“­ æœªæ‰¾åˆ°ä»»ä½•èµ„æº"
                    
                    formatted = f"ğŸ“¦ æ‰¾åˆ° {len(items)} ä¸ªèµ„æº:\n\n"
                    for item in items[:10]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                        # å®‰å…¨åœ°å¤„ç†å¯èƒ½ä¸ºNoneçš„item
                        if item is not None and isinstance(item, dict):
                            metadata = item.get("metadata", {})
                            status = item.get("status", {})
                            
                            # å®‰å…¨åœ°è·å–åç§°
                            name = "Unknown"
                            if isinstance(metadata, dict):
                                name = metadata.get("name", "Unknown")
                            elif "name" in item:
                                name = item.get("name", "Unknown")
                            
                            # å®‰å…¨åœ°è·å–å‘½åç©ºé—´
                            namespace = "default"
                            if isinstance(metadata, dict):
                                namespace = metadata.get("namespace", "default")
                            elif "namespace" in item:
                                namespace = item.get("namespace", "default")
                            
                            # å®‰å…¨åœ°è·å–çŠ¶æ€
                            phase = "Unknown"
                            if isinstance(status, dict):
                                phase = status.get("phase", "Unknown")
                            elif "phase" in item:
                                phase = item.get("phase", "Unknown")
                            elif "status" in item:
                                phase = item.get("status", "Unknown")
                            
                            formatted += f"â€¢ **{name}**\n"
                            formatted += f"  å‘½åç©ºé—´: {namespace}\n"
                            formatted += f"  çŠ¶æ€: {phase}\n\n"
                        else:
                            formatted += f"â€¢ **æ— æ•ˆèµ„æºé¡¹**\n"
                            formatted += f"  æ•°æ®: {str(item)[:50]}...\n\n"
                    
                    if len(items) > 10:
                        formatted += f"... è¿˜æœ‰ {len(items) - 10} ä¸ªèµ„æº\n"
                    
                    return formatted
                
                elif "pod_name" in result and "content" in result:
                    # æ—¥å¿—æ ¼å¼
                    return f"ğŸ“‹ **{result['pod_name']}** æ—¥å¿—:\n\n```\n{result['content']}\n```"
                
                elif "deployment_name" in result:
                    # æ‰©ç¼©å®¹ç»“æœæ ¼å¼
                    return f"ğŸ”„ æ‰©ç¼©å®¹å®Œæˆ:\n" + \
                           f"â€¢ éƒ¨ç½²åç§°: {result['deployment_name']}\n" + \
                           f"â€¢ å‘½åç©ºé—´: {result.get('namespace', 'default')}\n" + \
                           f"â€¢ å‰¯æœ¬æ•°: {result.get('previous_replicas', 0)} â†’ {result.get('target_replicas', 0)}\n" + \
                           f"â€¢ çŠ¶æ€: {'âœ… æˆåŠŸ' if result.get('success') else 'âŒ å¤±è´¥'}"
                
                else:
                    # é€šç”¨å­—å…¸æ ¼å¼
                    try:
                        return f"ğŸ“„ ç»“æœ:\n```json\n{json.dumps(result, ensure_ascii=False, indent=2)}\n```"
                    except Exception as json_error:
                        return f"ğŸ“„ ç»“æœ:\n{str(result)}"
            
            elif isinstance(result, list):
                return f"ğŸ“‹ åˆ—è¡¨ç»“æœ ({len(result)} é¡¹):\n" + \
                       "\n".join(f"â€¢ {item}" for item in result[:10] if item is not None)
            
            else:
                return f"ğŸ“„ ç»“æœ: {str(result)}"
        except Exception as e:
            logger.error(f"æ ¼å¼åŒ–å·¥å…·ç»“æœæ—¶å‡ºé”™: {e}")
            return f"ğŸ“„ å·¥å…·æ‰§è¡Œå®Œæˆï¼Œç»“æœç±»å‹: {type(result).__name__}"

    def _simplify_usage(self, usage_obj: Any) -> Optional[Dict[str, int]]:
        """å°†OpenAI/å…¼å®¹æä¾›å•†çš„usageç»“æ„ç®€åŒ–ä¸ºä»…åŒ…å«æ•´æ•°å­—æ®µï¼Œé¿å…Pydanticæ ¡éªŒå¤±è´¥ã€‚"""
        try:
            if not usage_obj:
                return None
            # å…ˆå°è¯•model_dump
            data = None
            try:
                data = usage_obj.model_dump()
            except Exception:
                # å›é€€åˆ°å±æ€§è¯»å–
                data = {
                    "prompt_tokens": getattr(usage_obj, 'prompt_tokens', None),
                    "completion_tokens": getattr(usage_obj, 'completion_tokens', None),
                    "total_tokens": getattr(usage_obj, 'total_tokens', None),
                }
            # ä»…ä¿ç•™ä¸‰ç±»å¸¸ç”¨æ•´æ•°å­—æ®µ
            simplified: Dict[str, int] = {}
            for key in ("prompt_tokens", "completion_tokens", "total_tokens"):
                val = data.get(key) if isinstance(data, dict) else None
                if isinstance(val, (int,)):
                    simplified[key] = val
                elif isinstance(val, float):
                    simplified[key] = int(val)
                elif isinstance(val, dict):
                    # æœ‰äº›æä¾›å•†ä¼šæŠŠ details æ”¾åœ¨å¯¹è±¡é‡Œï¼Œè¿™é‡Œå°½é‡å–å¸¸ç”¨å­—æ®µ
                    inner = val.get("total") or val.get("count")
                    if isinstance(inner, (int, float)):
                        simplified[key] = int(inner)
            return simplified or None
        except Exception:
            return None